# Environment & Control Interface Summary

This document captures the structured inputs and outputs owned by Track A so downstream agents can build data pipelines, models, and evaluation tooling without digging through implementation details.

## Environment Overview

- **Module**: `envs.core_env`
- **Primary class**: `NewtonMAEnv`
- **Configuration**: `EnvironmentConfig` dataclass parsed via `EnvironmentConfig.from_mapping`.
- **Agents**: fixed to two cooperative arms (`NUM_AGENTS = 2`).
- **Action space**: decimal position deltas + normalized gripper command (`ACTION_SIZE = 4`).
- **Seeding**: deterministic via `numpy.random.default_rng` seeded in `NewtonMAEnv.__init__`.

### Reset Lifecycle

```python
env = NewtonMAEnv({"task_name": "lift", "max_steps": 200, "seed": 42})
observations = env.reset("lift the cube together")
```

- `instruction`: arbitrary string stored in each observation under `"instruction"`.
- Task metadata is returned from the registered `TaskSpec.reset` implementation and propagated as `extras`.
- The environment caches per-step phase info (`self._current_phase`) and resets attachment state.

### Observation Schema

Per-agent observations (list length 2) are generated by `_build_observation` → `obs_i`. Each observation is a dictionary containing:

| Key | Type | Notes |
| --- | --- | --- |
| `agent_id` | `int` | 0 for left arm, 1 for right arm. |
| `task` | `str` | Task identifier from `TaskSpec`. |
| `phase` | `str` | Current high-level phase (first element of metadata phases when reset). |
| `instruction` | `str` | Natural-language instruction passed to `reset`. |
| `robot_state.position` | `np.ndarray[float32]` shape `(3,)` | Cartesian position of the agent’s end effector. |
| `robot_state.velocity` | `np.ndarray[float32]` shape `(3,)` | Cartesian velocity. |
| `robot_state.gripper` | `float` | Normalized closure in `[0.0, 1.0]`. |
| `peer_state.position` | `np.ndarray[float32]` shape `(3,)` | Peer arm end-effector position. |
| `peer_state.velocity` | `np.ndarray[float32]` shape `(3,)` | Peer arm velocity. |
| `peer_state.gripper` | `float` | Peer gripper closure. |
| `objects` | `dict[str, np.ndarray]` | Positions for all tracked objects. |
| `metadata` | `dict[str, Any]` | Copy of task metadata (phase progression, success flags, thresholds). |
| `rgb` | `np.ndarray[uint8]` shape `(48, 48, 3)` | Synthetic top-down rendering (placeholder until real cameras). |

All numpy arrays are created with `dtype=np.float64` inside the environment but may be cast downstream. Track B should normalize or convert as needed when writing episodes.

### Action Specification

- Actions are sequences (lists, tuples, or numpy arrays) convertible to 1-D numpy arrays.
- Expectation: `len(action) >= 4`. When shorter, `_apply_actions` pads with zeros.
- Layout: `[Δx, Δy, Δz, gripper]`
  - Translation deltas (meters) are clipped to `[-model.action_limit, model.action_limit]`.
  - Positions are then clamped within the workspace bounds defined in `SimulationModel`.
  - Gripper command is saturated to `[0.0, 1.0]`.
- Validation: `_validate_actions` ensures correct number of agents and vector-like inputs; raises descriptive `TypeError`/`ValueError`.

### Step Outputs

```python
next_obs, rewards, done, info = env.step(action_batch)
```

- `next_obs`: same schema as reset (length 2).
- `rewards`: list of two floats from the active `TaskSpec.reward` implementation.
- `done`: boolean; true when task success or `max_steps` reached.
- `info`: dictionary combining task info, previous/current phase, step counter, success flag, attachments, and `metadata` copy.

### Task Registry

- `envs.tasks.base` exposes:
  - `TaskSpec` protocol describing `build_scene`, `phases`, `reset`, `reward`, `success`, `scripted_action`, `info`.
  - `TaskMetadata` dataclass with attributes `phases: Sequence[str]` and `extras: Mapping[str, Any]`.
  - Registration helpers `register_task(name, cls)`, `deregister_task(name)`, `get_task(name)`, and `iter_registered_tasks()`.
- Concrete tasks:
  - `envs.tasks.lift.LiftTask`
  - `envs.tasks.handoff.HandOffTask`
  - `envs.tasks.drawer.DrawerTask`

Each module registers its task at import time, so `import envs.tasks.lift` (or `from envs import NewtonMAEnv`) is sufficient to populate the registry.

## Control Layer

### IK Utilities (`control/ik_utils.py`)

- `solve_ik(q_init, target_pose, limits=None, max_iters=100, atol=1e-3)` returns a clipped vector that moves toward the translation component of `target_pose`.
- `ee_pose_from_state(state, agent_id)` extracts a 7-DoF pose (position + quaternion placeholder).
- `plan_rendezvous` outputs mirrored grasp targets around an object.
- `gripper_command` clamps normalized ratios.
- `clamp_action` enforces maximum step norm.
- Helpers `_extract_position`, `_apply_limits`, `_ensure_vector` support conversions from `SimulationState` or mapping-like observations.

### Phase Machine (`control/phase_machine.py`)

- `PhaseMachine(phases: Sequence[str], timeouts: Mapping[str, int] | None = None)` manages scripted control flow.
- Methods:
  - `reset()` sets index to zero and clears counters.
  - `step(signals: Mapping[str, bool])` advances when `_should_advance` returns true.
  - `current` property exposes the active phase string.
  - `is_terminal` returns true after the final phase completes.
- `phase_signals_from_state(observation, info)` converts environment outputs to boolean triggers.

### Scripted Policies (`control/scripted/*.py`)

Each policy exposes:

- `waypoint_library(cfg: Mapping[str, Any]) -> dict[str, np.ndarray]`
- `scripted_policy(env: NewtonMAEnv, phase_machine: PhaseMachine, obs: list[dict[str, Any]]) -> list[np.ndarray]`

Policies consume the observation schema above, rely on `PhaseMachine` for progression, and return two actions matching the action specification. All scripted policies seed `numpy.random.default_rng` using the environment’s RNG for determinism.

## Deliverables for Downstream Tracks

- Sample scripted policy outputs are generated in `tests/test_scripted_policies.py`, demonstrating per-phase deterministic vectors.
- A static reference JSON (`tests/fixtures/scripted_policy_sample.json`) contains canonical lift-task actions for three phases; downstream pipelines can use it as a schema sanity check.
- Observation/action schema documented here should be used by Track B when validating `EpisodeWriter` inputs and dataset transforms.
- Task registry guarantees `get_task(name)` works for `"lift"`, `"handoff"`, and `"drawer"`; additional tasks must follow the same interface and update this reference.

For questions or extensions, refer back to `planning/phase_1.md` and `planning/phase_2.md` which detail the evolution plan for environment and control logic.
